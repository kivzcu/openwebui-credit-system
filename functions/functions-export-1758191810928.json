[
    {
        "id": "credit_management_charging_credits",
        "user_id": "d905e88b-549b-494e-95c8-ca3224fcaa2e",
        "name": "Credit management Charging credits",
        "type": "filter",
        "content": "\"\"\"\ntitle: Credit management Charging credits\nauthor: Miloslav Konop√≠k, DDVVY\nversion: 1.0\n\"\"\"\n\nimport os\nfrom pydantic import BaseModel, Field\nimport httpx\nimport tiktoken\nimport re\nfrom functools import partial\n\n# Translation table for i18n support\nTRANSLATIONS = {\n    \"cs-CZ\": {\n        \"failed_to_load_metadata\": \"Nepoda≈ôilo se naƒç√≠st metadata kredit≈Ø: {}\",\n        \"missing_user_model_data\": \"Chybƒõj√≠c√≠ data u≈æivatele nebo modelu. Str≈æen√≠ kredit≈Ø p≈ôeskoƒçeno.\",\n        \"free_model\": \"üÜì Bezplatn√Ω model - kredity ne√∫ƒçtov√°ny.\",\n        \"failed_to_deduct\": \"Nepoda≈ôilo se st√°hnout kredity: {}\",\n        \"insufficient_credits\": \"‚ö†Ô∏è Nedostatek kredit≈Ø! √öƒçtov√°no {actual_cost:.3f} z {full_cost:.3f} kredit≈Ø (chyb√≠ {shortage:.3f}) ‚Äì Z≈Østatek: {new_balance:.3f}\",\n        \"charged_credits\": \"üí≥ √öƒçtov√°no {actual_cost:.3f} kredit≈Ø ‚Äì Nov√Ω z≈Østatek: {new_balance:.3f}\",\n        \"cost_estimate\": \"‚ö†Ô∏è Cena je odhad.\",\n    },\n    \"en\": {  # Fallback language\n        \"failed_to_load_metadata\": \" Failed to load credit metadata: {}\",\n        \"missing_user_model_data\": \" Missing user or model data. Credit update skipped.\",\n        \"free_model\": \"üÜì Free model - no credits charged.\",\n        \"failed_to_deduct\": \" Failed to deduct credits: {}\",\n        \"insufficient_credits\": \"‚ö†Ô∏è Insufficient credits! Charged {actual_cost:.3f} of {full_cost:.3f} credits (short by {shortage:.3f}) ‚Äì Balance: {new_balance:.3f}\",\n        \"charged_credits\": \"üí≥ Charged {actual_cost:.3f} credits ‚Äì New balance: {new_balance:.3f}\",\n        \"cost_estimate\": \"‚ö†Ô∏è The cost is an estimate.\",\n    },\n}\n\n\nclass Filter:\n    def _count_tokens_tiktoken(self, text: str, encoding_name: str) -> int:\n        \"\"\"Counts tokens using a specified tiktoken encoding.\"\"\"\n        encoding = tiktoken.get_encoding(encoding_name)\n        return len(encoding.encode(text))\n\n    def _count_tokens_anthropic_dummy(self, text: str) -> int:\n        \"\"\"\n        Example of a dummy counting function for a different library.\n        e.g., from anthropic import Anthropic; client = Anthropic(); client.count_tokens(text)\n        \"\"\"\n        # Dummy implementation: count words\n        return len(text.split())\n\n    class Valves(BaseModel):\n        show_status: bool = Field(\n            default=True, description=\"Zobrazit info o str≈æen√≠ kredit≈Ø\"\n        )\n        credits_api_protocol: str = Field(\n            default=\"https\", description=\"API protocol (http or https)\"\n        )\n        credits_api_host: str = Field(\n            default=\"147.228.121.27:8000\", description=\"API host and port\"\n        )\n        ssl_verify: bool = Field(default=False, description=\"Verify SSL certificates\")\n        api_key: str = Field(\n            default=\"vY97Yvh6qKywm8xE-ErTGfUofV0t1BiZ36wR3lLNHIY\",\n            description=\"API key for authentication\",\n        )\n\n    def __init__(self):\n        self.valves = self.Valves()\n        self.estimation_warning = \"\"\n        # Map of model name patterns to their respective token counting functions.\n        self.COUNT_FUNCTIONS = {\n            r\"^(gpt-4\\.1|4o-mini|o4)\": partial(\n                self._count_tokens_tiktoken, encoding_name=\"o200k_base\"\n            ),\n            r\"^(claude-.*)\": self._count_tokens_anthropic_dummy,\n        }\n\n    def _get_user_language(self, body):\n        \"\"\"Extract user language from body metadata\"\"\"\n        try:\n            return (\n                body.get(\"metadata\", {})\n                .get(\"variables\", {})\n                .get(\"{{USER_LANGUAGE}}\", \"en\")\n            )\n        except:\n            return \"en\"\n\n    def _translate(self, key, lang=\"en\", **kwargs):\n        \"\"\"Get translated string for given key and language\"\"\"\n        # Get the translation for the specific language, fallback to English\n        translation = TRANSLATIONS.get(lang, TRANSLATIONS[\"en\"]).get(\n            key, TRANSLATIONS[\"en\"].get(key, key)\n        )\n\n        # Ensure we have a valid translation string\n        if translation is None:\n            translation = key\n\n        # Format the translation with any provided kwargs\n        try:\n            return translation.format(**kwargs)\n        except:\n            return translation\n\n    def get_token_count(self, text: str, model_name: str) -> int:\n        \"\"\"\n        Returns the token count for a given text and model.\n        \"\"\"\n        # Check for a matching counting function for special cases\n        for pattern, func in self.COUNT_FUNCTIONS.items():\n            if re.match(pattern, model_name):\n                return func(text)\n\n        # If no special mapping, try getting encoding from model name\n        try:\n            encoding = tiktoken.encoding_for_model(model_name)\n            return len(encoding.encode(text))\n        except KeyError:\n            self.estimation_warning = \"‚ö†Ô∏è The cost is an estimate.\"\n            encoding = tiktoken.get_encoding(\"cl100k_base\")\n            return len(encoding.encode(text))\n\n    def count_tokens(self, msg: dict, model_name: str) -> int:\n        content = msg.get(\"content\", \"\")\n        total_tokens = 0\n\n        if isinstance(content, list):\n            # Handle multi-modal content\n            for item in content:\n                if item.get(\"type\") == \"text\":\n                    text = item.get(\"text\", \"\")\n                    total_tokens += self.get_token_count(text, model_name)\n        elif isinstance(content, str):\n            # Handle text-only content\n            total_tokens = self.get_token_count(content, model_name)\n        else:\n            # Handle unexpected content types\n            total_tokens = 0\n\n        return total_tokens\n\n    async def outlet(\n        self, body, __user__=None, __event_emitter__=None, __event_call__=None\n    ):\n        credits_api_base_url = f\"{self.valves.credits_api_protocol}://{self.valves.credits_api_host}/api/credits\"\n        if not __user__:\n            return body\n\n        user_id = __user__.get(\"id\")\n        model_name = body.get(\"model\", \"gpt-3.5-turbo\")\n        self.estimation_warning = \"\"\n\n        # Get user language for translations\n        user_lang = self._get_user_language(body)\n\n        messages = body.get(\"messages\", [])\n        if not messages:\n            return body\n\n        # print(body)\n\n        # The last message is the completion, the rest are the prompt\n        completion_message = messages[-1]\n\n        usage = completion_message.get(\"usage\")\n        if usage and \"prompt_tokens\" in usage and \"completion_tokens\" in usage:\n            prompt_tokens = usage[\"prompt_tokens\"]\n            completion_tokens = usage[\"completion_tokens\"]\n            self.estimation_warning = \"\"  # Exact cost, no warning\n            actor = \"model-usage\"\n\n            # Extract cached tokens and reasoning tokens\n            cached_tokens = usage.get(\"prompt_tokens_details\", {}).get(\n                \"cached_tokens\", 0\n            )\n            reasoning_tokens = usage.get(\"completion_tokens_details\", {}).get(\n                \"reasoning_tokens\", 0\n            )\n\n        else:\n            # Fallback to manual counting if usage is not available\n            prompt_messages = messages[:-1]\n            prompt_tokens = sum(\n                self.count_tokens(msg, model_name) for msg in prompt_messages\n            )\n            completion_tokens = self.count_tokens(completion_message, model_name)\n            cached_tokens = 0  # Not available in manual counting\n            reasoning_tokens = 0  # Not available in manual counting\n            if self.estimation_warning:\n                actor = \"estimate-count\"\n            else:\n                actor = \"manual-count\"\n\n        try:\n            # Set up headers with API key\n            headers = {\"X-API-Key\": self.valves.api_key} if self.valves.api_key else {}\n\n            print(f\"{credits_api_base_url}/model/{model_name}, headers={headers}\")\n\n            async with httpx.AsyncClient(verify=self.valves.ssl_verify) as client:\n                # Use optimized endpoints - get only the specific user and model we need\n                user_res = await client.get(\n                    f\"{credits_api_base_url}/user/{user_id}\", headers=headers\n                )\n                model_res = await client.get(\n                    f\"{credits_api_base_url}/model/{model_name}\", headers=headers\n                )\n                user_res.raise_for_status()\n                model_res.raise_for_status()\n                user_data = user_res.json()\n                model_data = model_res.json()\n        except Exception as e:\n            if self.valves.show_status and __event_emitter__:\n                await __event_emitter__(\n                    {\n                        \"type\": \"status\",\n                        \"data\": {\n                            \"description\": self._translate(\n                                \"failed_to_load_metadata\", user_lang\n                            ).format(str(e)),\n                            \"done\": True,\n                        },\n                    }\n                )\n            return body\n\n        if not model_data or not user_data:\n            if self.valves.show_status and __event_emitter__:\n                await __event_emitter__(\n                    {\n                        \"type\": \"status\",\n                        \"data\": {\n                            \"description\": self._translate(\n                                \"missing_user_model_data\", user_lang\n                            ),\n                            \"done\": True,\n                        },\n                    }\n                )\n            return body\n\n        # Check if model is free\n        is_free = model_data.get(\"is_free\", False)\n\n        if is_free:\n            # For free models, skip credit deduction\n            if self.valves.show_status and __event_emitter__:\n                await __event_emitter__(\n                    {\n                        \"type\": \"status\",\n                        \"data\": {\n                            \"description\": self._translate(\"free_model\", user_lang),\n                            \"done\": True,\n                        },\n                    }\n                )\n            return body\n\n        context_price = float(model_data.get(\"context_price\", 0))\n        generation_price = float(model_data.get(\"generation_price\", 0))\n        credits = float(user_data.get(\"credits\", 0))\n\n        cost = prompt_tokens * context_price + completion_tokens * generation_price\n\n        try:\n            # Set up headers with API key\n            headers = {\"X-API-Key\": self.valves.api_key} if self.valves.api_key else {}\n\n            async with httpx.AsyncClient(verify=self.valves.ssl_verify) as client:\n                # Use the new optimized deduction endpoint\n                deduction_res = await client.post(\n                    f\"{credits_api_base_url}/deduct-tokens\",\n                    json={\n                        \"user_id\": user_id,\n                        \"model_id\": model_name,\n                        \"prompt_tokens\": prompt_tokens,\n                        \"completion_tokens\": completion_tokens,\n                        \"cached_tokens\": cached_tokens,\n                        \"reasoning_tokens\": reasoning_tokens,\n                        \"actor\": actor,\n                    },\n                    headers=headers,\n                )\n                deduction_res.raise_for_status()\n                result = deduction_res.json()\n                new_balance = result.get(\"new_balance\", 0)\n                actual_cost = result.get(\"deducted\", 0)\n                full_cost = result.get(\"cost\", 0)\n        except Exception as e:\n            if self.valves.show_status and __event_emitter__:\n                await __event_emitter__(\n                    {\n                        \"type\": \"status\",\n                        \"data\": {\n                            \"description\": self._translate(\n                                \"failed_to_deduct\", user_lang\n                            ).format(str(e)),\n                            \"done\": True,\n                        },\n                    }\n                )\n            return body\n\n        if self.valves.show_status and __event_emitter__:\n            # Check if user had insufficient funds\n            if actual_cost < full_cost:\n                # Insufficient funds scenario\n                shortage = full_cost - actual_cost\n                description = self._translate(\n                    \"insufficient_credits\",\n                    user_lang,\n                    actual_cost=actual_cost,\n                    full_cost=full_cost,\n                    shortage=shortage,\n                    new_balance=new_balance,\n                )\n            else:\n                # Normal scenario - full payment\n                description = self._translate(\n                    \"charged_credits\",\n                    user_lang,\n                    actual_cost=actual_cost,\n                    new_balance=new_balance,\n                )\n\n            if self.estimation_warning:\n                estimate_warning = self._translate(\"cost_estimate\", user_lang)\n                description = estimate_warning + \"<br/>\" + description\n\n            await __event_emitter__(\n                {\n                    \"type\": \"status\",\n                    \"data\": {\n                        \"description\": description,\n                        \"done\": True,\n                    },\n                }\n            )\n\n        return body\n",
        "meta": {
            "description": "Charging credits.",
            "manifest": {
                "title": "Credit management Charging credits",
                "author": "Miloslav Konop√≠k, DDVVY",
                "version": "1.0"
            }
        },
        "is_active": true,
        "is_global": true,
        "updated_at": 1757951057,
        "created_at": 1757946354
    },
    {
        "id": "credit_management_enough_credits",
        "user_id": "d905e88b-549b-494e-95c8-ca3224fcaa2e",
        "name": "Credit management  enough credits",
        "type": "filter",
        "content": "\"\"\"\ntitle: Credit management  enough credits\nauthor: Miloslav Konop√≠k, DDVVY\nversion: 1.0\n\"\"\"\n\nimport os\nfrom pydantic import BaseModel, Field\nimport httpx\n\n# Translation table for i18n support\nTRANSLATIONS = {\n    \"cs-CZ\": {\n        \"failed_to_load_data\": \"Nepoda≈ôilo se naƒç√≠st data kredit≈Ø: {}\",\n        \"user_not_found\": \"Data u≈æivatele nenalezena.\",\n        \"model_not_found\": \"Model nenalezen v cen√≠ku.\",\n        \"free_model\": \"üÜì Bezplatn√Ω model - kredity ne√∫ƒçtov√°ny.\",\n        \"insufficient_prompt_blocked\": \"Nedostatek kredit≈Ø ‚Äì prompt zablokov√°n.\",\n        \"insufficient_credits_error\": \"Nem√°te dostatek kredit≈Ø: {} k dispozici, minim√°lnƒõ {} pot≈ôeba.\",\n    },\n    \"en\": {  # Fallback language\n        \"failed_to_load_data\": \"Unable to load credit data: {}\",\n        \"user_not_found\": \"User data not found.\",\n        \"model_not_found\": \"Model not found in cost list.\",\n        \"free_model\": \"üÜì Free model - no credits charged.\",\n        \"insufficient_prompt_blocked\": \"Insufficient credits ‚Äì prompt blocked.\",\n        \"insufficient_credits_error\": \"You do not have enough credits: {} available, minimum {} required.\",\n    },\n}\n\n\n# If not available in your project, define your own exception:\nclass FilterException(Exception):\n    pass\n\n\nclass Filter:\n    \"\"\"\n    Filters prompts based on user's available credits. Blocks prompt execution if\n    the estimated cost exceeds available user credits. Fetches cost and user data\n    from a backend API.\n    \"\"\"\n\n    class Valves(BaseModel):\n        show_status: bool = Field(\n            default=True, description=\"Show credit status message to the user.\"\n        )\n        credits_api_protocol: str = Field(\n            default=\"http\", description=\"API protocol (http or https)\"\n        )\n        credits_api_host: str = Field(\n            default=\"localhost:8000\", description=\"API host and port\"\n        )\n        ssl_verify: bool = Field(default=False, description=\"Verify SSL certificates\")\n        api_key: str = Field(default=\"\", description=\"API key for authentication\")\n\n    def __init__(self):\n        self.valves = self.Valves()\n\n    def _get_user_language(self, body):\n        \"\"\"Extract user language from body metadata\"\"\"\n        try:\n            return (\n                body.get(\"metadata\", {})\n                .get(\"variables\", {})\n                .get(\"{{USER_LANGUAGE}}\", \"en\")\n            )\n        except:\n            return \"en\"\n\n    def _translate(self, key, lang=\"en\", **kwargs):\n        \"\"\"Get translated string for given key and language\"\"\"\n        # Get the translation for the specific language, fallback to English\n        translation = TRANSLATIONS.get(lang, TRANSLATIONS[\"en\"]).get(\n            key, TRANSLATIONS[\"en\"].get(key, key)\n        )\n\n        # Ensure we have a valid translation string\n        if translation is None:\n            translation = key\n\n        # Format the translation with any provided kwargs\n        try:\n            return translation.format(**kwargs)\n        except:\n            return translation\n\n    def format_credit_amount(self, amount):\n        \"\"\"Format credit amount to avoid scientific notation and excessive precision\"\"\"\n        if amount == 0:\n            return \"0\"\n\n        # Convert to decimal string to avoid scientific notation\n        decimal_str = f\"{amount:.10f}\"\n\n        # Remove trailing zeros and decimal point if not needed\n        if \".\" in decimal_str:\n            decimal_str = decimal_str.rstrip(\"0\").rstrip(\".\")\n\n        # If the result is empty (very small number), show minimum precision\n        if not decimal_str or decimal_str == \"0.\":\n            return f\"{amount:.8f}\".rstrip(\"0\").rstrip(\".\")\n\n        return decimal_str\n\n    async def inlet(\n        self, body, __user__=None, __event_emitter__=None, __event_call__=None\n    ):\n        credits_api_base_url = f\"{self.valves.credits_api_protocol}://{self.valves.credits_api_host}/api/credits\"\n        user_id = __user__.get(\"id\")\n        model_name = body.get(\"model\")\n        prompt_text = body[\"messages\"][-1][\"content\"]\n        prompt_tokens = body.get(\"prompt_tokens\") or max(len(prompt_text) // 4, 1)\n\n        # Get user language for translations\n        user_lang = self._get_user_language(body)\n\n        try:\n            # Set up headers with API key\n            headers = {\"X-API-Key\": self.valves.api_key} if self.valves.api_key else {}\n\n            async with httpx.AsyncClient(verify=self.valves.ssl_verify) as client:\n                # Use optimized endpoints - get only specific user and model\n                user_res = await client.get(\n                    f\"{credits_api_base_url}/user/{user_id}\", headers=headers\n                )\n                model_res = await client.get(\n                    f\"{credits_api_base_url}/model/{model_name}\", headers=headers\n                )\n                user_res.raise_for_status()\n                model_res.raise_for_status()\n                user_data = user_res.json()\n                model_data = model_res.json()\n        except Exception as e:\n            body[\"messages\"][-1][\n                \"content\"\n            ] += f\"\\n\\n{self._translate('failed_to_load_data', user_lang).format(str(e))}\"\n            return body\n\n        user_data = user_data\n        if not user_data:\n            body[\"messages\"][-1][\n                \"content\"\n            ] += f\"\\n\\n{self._translate('user_not_found', user_lang)}\"\n            return body\n\n        model_data = model_data\n        if not model_data:\n            body[\"messages\"][-1][\n                \"content\"\n            ] += f\"\\n\\n{self._translate('model_not_found', user_lang)}\"\n            return body\n\n        # Check if model is free - if so, allow the request without credit check\n        is_free = model_data.get(\"is_free\", False)\n        if is_free:\n            if self.valves.show_status and __event_emitter__:\n                await __event_emitter__(\n                    {\n                        \"type\": \"status\",\n                        \"data\": {\n                            \"description\": self._translate(\"free_model\", user_lang),\n                            \"done\": True,\n                        },\n                    }\n                )\n            return body\n\n        context_price = model_data.get(\"context_price\", 0)\n        cost = prompt_tokens * context_price\n        credits = user_data.get(\"credits\", 0)\n\n        if credits < cost:\n            if self.valves.show_status and __event_emitter__:\n                await __event_emitter__(\n                    {\n                        \"type\": \"status\",\n                        \"data\": {\n                            \"description\": self._translate(\n                                \"insufficient_prompt_blocked\", user_lang\n                            ),\n                            \"done\": True,\n                        },\n                    }\n                )\n\n            raise FilterException(\n                self._translate(\"insufficient_credits_error\", user_lang).format(\n                    self.format_credit_amount(credits), self.format_credit_amount(cost)\n                )\n            )\n\n        # Prompt is allowed, but no success message is emitted\n        return body\n",
        "meta": {
            "description": "Filter checking enought credits.",
            "manifest": {
                "title": "Credit management  enough credits",
                "author": "Miloslav Konop√≠k, DDVVY",
                "version": "1.0"
            }
        },
        "is_active": true,
        "is_global": true,
        "updated_at": 1757951255,
        "created_at": 1757946422
    },
    {
        "id": "google_gemini_pipeline",
        "user_id": "d905e88b-549b-494e-95c8-ca3224fcaa2e",
        "name": "Google Gemini Pipeline",
        "type": "pipe",
        "content": "\"\"\"\ntitle: Google Gemini Pipeline\nauthor: owndev, olivier-lacroix\nauthor_url: https://github.com/owndev/\nproject_url: https://github.com/owndev/Open-WebUI-Functions\nfunding_url: https://github.com/sponsors/owndev\nversion: 1.3.3\nlicense: Apache License 2.0\ndescription: A manifold pipeline for interacting with Google Gemini models, including dynamic model specification, streaming responses, and flexible error handling.\nfeatures:\n  - Asynchronous API calls for better performance\n  - Model caching to reduce API calls\n  - Dynamic model specification with automatic prefix stripping\n  - Streaming response handling with safety checks\n  - Support for multimodal input (text and images)\n  - Flexible error handling and logging\n  - Integration with Google Generative AI or Vertex AI API for content generation\n  - Support for various generation parameters (temperature, max tokens, etc.)\n  - Customizable safety settings based on environment variables\n  - Encrypted storage of sensitive API keys\n  - Grounding with Google search\n  - Native tool calling support\n\"\"\"\n\nimport os\nimport inspect\nfrom functools import update_wrapper\nimport re\nimport time\nimport asyncio\nimport base64\nimport hashlib\nimport logging\nfrom google import genai\nfrom google.genai import types\nfrom google.genai.errors import ClientError, ServerError, APIError\nfrom typing import List, Union, Optional, Dict, Any, Tuple, AsyncIterator, Callable\nfrom pydantic_core import core_schema\nfrom pydantic import BaseModel, Field, GetCoreSchemaHandler\nfrom cryptography.fernet import Fernet, InvalidToken\nfrom open_webui.env import SRC_LOG_LEVELS\n\n\n# Simplified encryption implementation with automatic handling\nclass EncryptedStr(str):\n    \"\"\"A string type that automatically handles encryption/decryption\"\"\"\n\n    @classmethod\n    def _get_encryption_key(cls) -> Optional[bytes]:\n        \"\"\"\n        Generate encryption key from WEBUI_SECRET_KEY if available\n        Returns None if no key is configured\n        \"\"\"\n        secret = os.getenv(\"WEBUI_SECRET_KEY\")\n        if not secret:\n            return None\n\n        hashed_key = hashlib.sha256(secret.encode()).digest()\n        return base64.urlsafe_b64encode(hashed_key)\n\n    @classmethod\n    def encrypt(cls, value: str) -> str:\n        \"\"\"\n        Encrypt a string value if a key is available\n        Returns the original value if no key is available\n        \"\"\"\n        if not value or value.startswith(\"encrypted:\"):\n            return value\n\n        key = cls._get_encryption_key()\n        if not key:  # No encryption if no key\n            return value\n\n        f = Fernet(key)\n        encrypted = f.encrypt(value.encode())\n        return f\"encrypted:{encrypted.decode()}\"\n\n    @classmethod\n    def decrypt(cls, value: str) -> str:\n        \"\"\"\n        Decrypt an encrypted string value if a key is available\n        Returns the original value if no key is available or decryption fails\n        \"\"\"\n        if not value or not value.startswith(\"encrypted:\"):\n            return value\n\n        key = cls._get_encryption_key()\n        if not key:  # No decryption if no key\n            return value[len(\"encrypted:\") :]  # Return without prefix\n\n        try:\n            encrypted_part = value[len(\"encrypted:\") :]\n            f = Fernet(key)\n            decrypted = f.decrypt(encrypted_part.encode())\n            return decrypted.decode()\n        except (InvalidToken, Exception):\n            return value\n\n    # Pydantic integration\n    @classmethod\n    def __get_pydantic_core_schema__(\n        cls, _source_type: Any, _handler: GetCoreSchemaHandler\n    ) -> core_schema.CoreSchema:\n        return core_schema.union_schema(\n            [\n                core_schema.is_instance_schema(cls),\n                core_schema.chain_schema(\n                    [\n                        core_schema.str_schema(),\n                        core_schema.no_info_plain_validator_function(\n                            lambda value: cls(cls.encrypt(value) if value else value)\n                        ),\n                    ]\n                ),\n            ],\n            serialization=core_schema.plain_serializer_function_ser_schema(\n                lambda instance: str(instance)\n            ),\n        )\n\n    def get_decrypted(self) -> str:\n        \"\"\"Get the decrypted value\"\"\"\n        return self.decrypt(self)\n\n\nclass Pipe:\n    \"\"\"\n    Pipeline for interacting with Google Gemini models.\n    \"\"\"\n\n    # Configuration valves for the pipeline\n    class Valves(BaseModel):\n        GOOGLE_API_KEY: EncryptedStr = Field(\n            default=os.getenv(\"GOOGLE_API_KEY\", \"\"),\n            description=\"API key for Google Generative AI (used if USE_VERTEX_AI is false).\",\n        )\n        USE_VERTEX_AI: bool = Field(\n            default=os.getenv(\"GOOGLE_GENAI_USE_VERTEXAI\", \"false\").lower() == \"true\",\n            description=\"Whether to use Google Cloud Vertex AI instead of the Google Generative AI API.\",\n        )\n        VERTEX_PROJECT: str | None = Field(\n            default=os.getenv(\"GOOGLE_CLOUD_PROJECT\"),\n            description=\"The Google Cloud project ID to use with Vertex AI.\",\n        )\n        VERTEX_LOCATION: str = Field(\n            default=os.getenv(\"GOOGLE_CLOUD_LOCATION\", \"global\"),\n            description=\"The Google Cloud region to use with Vertex AI.\",\n        )\n        USE_PERMISSIVE_SAFETY: bool = Field(\n            default=os.getenv(\"USE_PERMISSIVE_SAFETY\", \"false\").lower() == \"true\",\n            description=\"Use permissive safety settings for content generation.\",\n        )\n        MODEL_CACHE_TTL: int = Field(\n            default=int(os.getenv(\"GOOGLE_MODEL_CACHE_TTL\", \"600\")),\n            description=\"Time in seconds to cache the model list before refreshing\",\n        )\n        RETRY_COUNT: int = Field(\n            default=int(os.getenv(\"GOOGLE_RETRY_COUNT\", \"2\")),\n            description=\"Number of times to retry API calls on temporary failures\",\n        )\n\n    def __init__(self):\n        \"\"\"Initializes the Pipe instance and configures the genai library.\"\"\"\n        self.valves = self.Valves()\n        self.name: str = \"Google Gemini: \"\n\n        # Setup logging\n        self.log = logging.getLogger(\"google_ai.pipe\")\n        self.log.setLevel(SRC_LOG_LEVELS.get(\"OPENAI\", logging.INFO))\n\n        # Model cache\n        self._model_cache: Optional[List[Dict[str, str]]] = None\n        self._model_cache_time: float = 0\n\n    def _get_client(self) -> genai.Client:\n        \"\"\"\n        Validates API credentials and returns a genai.Client instance.\n        \"\"\"\n        self._validate_api_key()\n\n        if self.valves.USE_VERTEX_AI:\n            self.log.debug(\n                f\"Initializing Vertex AI client (Project: {self.valves.VERTEX_PROJECT}, Location: {self.valves.VERTEX_LOCATION})\"\n            )\n            return genai.Client(\n                vertexai=True,\n                project=self.valves.VERTEX_PROJECT,\n                location=self.valves.VERTEX_LOCATION,\n            )\n        else:\n            self.log.debug(\"Initializing Google Generative AI client with API Key\")\n            return genai.Client(api_key=self.valves.GOOGLE_API_KEY.get_decrypted())\n\n    def _validate_api_key(self) -> None:\n        \"\"\"\n        Validates that the necessary Google API credentials are set.\n\n        Raises:\n            ValueError: If the required credentials are not set.\n        \"\"\"\n        if self.valves.USE_VERTEX_AI:\n            if not self.valves.VERTEX_PROJECT:\n                self.log.error(\"USE_VERTEX_AI is true, but VERTEX_PROJECT is not set.\")\n                raise ValueError(\n                    \"VERTEX_PROJECT is not set. Please provide the Google Cloud project ID.\"\n                )\n            # For Vertex AI, location has a default, so project is the main thing to check.\n            # Actual authentication will be handled by ADC or environment.\n            self.log.debug(\n                \"Using Vertex AI. Ensure ADC or service account is configured.\"\n            )\n        else:\n            if not self.valves.GOOGLE_API_KEY:\n                self.log.error(\"GOOGLE_API_KEY is not set (and not using Vertex AI).\")\n                raise ValueError(\n                    \"GOOGLE_API_KEY is not set. Please provide the API key in the environment variables or valves.\"\n                )\n            self.log.debug(\"Using Google Generative AI API with API Key.\")\n\n    def strip_prefix(self, model_name: str) -> str:\n        \"\"\"\n        Extract the model identifier using regex, handling various naming conventions.\n        e.g., \"google_gemini_pipeline.gemini-2.5-flash-preview-04-17\" -> \"gemini-2.5-flash-preview-04-17\"\n        e.g., \"models/gemini-1.5-flash-001\" -> \"gemini-1.5-flash-001\"\n        e.g., \"publishers/google/models/gemini-1.5-pro\" -> \"gemini-1.5-pro\"\n        \"\"\"\n        # Use regex to remove everything up to and including the last '/' or the first '.'\n        stripped = re.sub(r\"^(?:.*/|[^.]*\\.)\", \"\", model_name)\n        return stripped\n\n    def get_google_models(self, force_refresh: bool = False) -> List[Dict[str, str]]:\n        \"\"\"\n        Retrieve available Google models suitable for content generation.\n        Uses caching to reduce API calls.\n\n        Args:\n            force_refresh: Whether to force refreshing the model cache\n\n        Returns:\n            List of dictionaries containing model id and name.\n        \"\"\"\n        # Check cache first\n        current_time = time.time()\n        if (\n            not force_refresh\n            and self._model_cache is not None\n            and (current_time - self._model_cache_time) < self.valves.MODEL_CACHE_TTL\n        ):\n            self.log.debug(\"Using cached model list\")\n            return self._model_cache\n\n        try:\n            client = self._get_client()\n            self.log.debug(\"Fetching models from Google API\")\n            models = client.models.list()\n            available_models = []\n            for model in models:\n                actions = model.supported_actions\n                if actions is None or \"generateContent\" in actions:\n                    available_models.append(\n                        {\n                            \"id\": self.strip_prefix(model.name),\n                            \"name\": model.display_name or self.strip_prefix(model.name),\n                        }\n                    )\n\n            model_map = {model[\"id\"]: model for model in available_models}\n\n            # Filter map to only include models starting with 'gemini-'\n            filtered_models = {\n                k: v for k, v in model_map.items() if k.startswith(\"gemini-\")\n            }\n\n            # Update cache\n            self._model_cache = list(filtered_models.values())\n            self._model_cache_time = current_time\n            self.log.debug(f\"Found {len(self._model_cache)} Gemini models\")\n            return self._model_cache\n\n        except Exception as e:\n            self.log.exception(f\"Could not fetch models from Google: {str(e)}\")\n            # Return a specific error entry for the UI\n            return [{\"id\": \"error\", \"name\": f\"Could not fetch models: {str(e)}\"}]\n\n    def pipes(self) -> List[Dict[str, str]]:\n        \"\"\"\n        Returns a list of available Google Gemini models for the UI.\n\n        Returns:\n            List of dictionaries containing model id and name.\n        \"\"\"\n        try:\n            self.name = \"Google Gemini: \"\n            return self.get_google_models()\n        except ValueError as e:\n            # Handle the case where API key is missing during pipe listing\n            self.log.error(f\"Error during pipes listing (validation): {e}\")\n            return [{\"id\": \"error\", \"name\": str(e)}]\n        except Exception as e:\n            # Handle other potential errors during model fetching\n            self.log.exception(\n                f\"An unexpected error occurred during pipes listing: {str(e)}\"\n            )\n            return [{\"id\": \"error\", \"name\": f\"An unexpected error occurred: {str(e)}\"}]\n\n    def _prepare_model_id(self, model_id: str) -> str:\n        \"\"\"\n        Prepare and validate the model ID for use with the API.\n\n        Args:\n            model_id: The original model ID from the user\n\n        Returns:\n            Properly formatted model ID\n\n        Raises:\n            ValueError: If the model ID is invalid or unsupported\n        \"\"\"\n        original_model_id = model_id\n        model_id = self.strip_prefix(model_id)\n\n        # If the model ID doesn't look like a Gemini model, try to find it by name\n        if not model_id.startswith(\"gemini-\"):\n            models_list = self.get_google_models()\n            found_model = next(\n                (m[\"id\"] for m in models_list if m[\"name\"] == original_model_id), None\n            )\n            if found_model and found_model.startswith(\"gemini-\"):\n                model_id = found_model\n                self.log.debug(\n                    f\"Mapped model name '{original_model_id}' to model ID '{model_id}'\"\n                )\n            else:\n                # If we still don't have a valid ID, raise an error\n                if not model_id.startswith(\"gemini-\"):\n                    self.log.error(\n                        f\"Invalid or unsupported model ID: '{original_model_id}'\"\n                    )\n                    raise ValueError(\n                        f\"Invalid or unsupported Google model ID or name: '{original_model_id}'\"\n                    )\n\n        return model_id\n\n    def _prepare_content(\n        self, messages: List[Dict[str, Any]]\n    ) -> Tuple[List[Dict[str, Any]], Optional[str]]:\n        \"\"\"\n        Prepare messages content for the API and extract system message if present.\n\n        Args:\n            messages: List of message objects from the request\n\n        Returns:\n            Tuple of (prepared content list, system message string or None)\n        \"\"\"\n        # Extract system message\n        system_message = next(\n            (msg[\"content\"] for msg in messages if msg.get(\"role\") == \"system\"),\n            None,\n        )\n\n        # Prepare contents for the API\n        contents = []\n        for message in messages:\n            role = message.get(\"role\")\n            if role == \"system\":\n                continue  # Skip system messages, handled separately\n\n            content = message.get(\"content\", \"\")\n            parts = []\n\n            # Handle different content types\n            if isinstance(content, list):  # Multimodal content\n                parts.extend(self._process_multimodal_content(content))\n            elif isinstance(content, str):  # Plain text content\n                parts.append({\"text\": content})\n            else:\n                self.log.warning(f\"Unsupported message content type: {type(content)}\")\n                continue  # Skip unsupported content\n\n            # Map roles: 'assistant' -> 'model', 'user' -> 'user'\n            api_role = \"model\" if role == \"assistant\" else \"user\"\n            if parts:  # Only add if there are parts\n                contents.append({\"role\": api_role, \"parts\": parts})\n\n        return contents, system_message\n\n    def _process_multimodal_content(\n        self, content_list: List[Dict[str, Any]]\n    ) -> List[Dict[str, Any]]:\n        \"\"\"\n        Process multimodal content (text and images).\n\n        Args:\n            content_list: List of content items\n\n        Returns:\n            List of processed parts for the Gemini API\n        \"\"\"\n        parts = []\n\n        for item in content_list:\n            if item.get(\"type\") == \"text\":\n                parts.append({\"text\": item.get(\"text\", \"\")})\n            elif item.get(\"type\") == \"image_url\":\n                image_url = item.get(\"image_url\", {}).get(\"url\", \"\")\n\n                if image_url.startswith(\"data:image\"):\n                    # Handle base64 encoded image data\n                    try:\n                        header, encoded = image_url.split(\",\", 1)\n                        mime_type = header.split(\":\")[1].split(\";\")[0]\n\n                        # Basic validation for image types\n                        if mime_type not in [\n                            \"image/jpeg\",\n                            \"image/png\",\n                            \"image/webp\",\n                            \"image/heic\",\n                            \"image/heif\",\n                        ]:\n                            self.log.warning(\n                                f\"Unsupported image mime type: {mime_type}\"\n                            )\n                            parts.append(\n                                {\"text\": f\"[Image type {mime_type} not supported]\"}\n                            )\n                            continue\n\n                        parts.append(\n                            {\n                                \"inline_data\": {\n                                    \"mime_type\": mime_type,\n                                    \"data\": encoded,\n                                }\n                            }\n                        )\n                    except Exception as img_ex:\n                        self.log.exception(f\"Could not parse image data URL: {img_ex}\")\n                        parts.append({\"text\": \"[Image data could not be processed]\"})\n                else:\n                    # Gemini API doesn't directly support image URLs\n                    self.log.warning(f\"Direct image URLs not supported: {image_url}\")\n                    parts.append({\"text\": f\"[Image URL not processed: {image_url}]\"})\n\n        return parts\n\n    @staticmethod\n    def _create_tool(tool_def):\n        \"\"\"OpenwebUI tool is a functools.partial coroutine, which genai does not support directly.\n        See https://github.com/googleapis/python-genai/issues/907\n\n        This function wraps the tool into a callable that can be used with genai.\n        In particular, it sets the signature of the function properly,\n        removing any frozen keyword arguments (extra_params).\n        \"\"\"\n        bound_callable = tool_def[\"callable\"]\n\n        # Create a wrapper for bound_callable, which is always async\n        async def wrapper(*args, **kwargs):\n            return await bound_callable(*args, **kwargs)\n\n        # Remove 'frozen' keyword arguments (extra_params) from the signature\n        original_sig = inspect.signature(bound_callable)\n        frozen_kwargs = {\n            \"__event_emitter__\",\n            \"__event_call__\",\n            \"__user__\",\n            \"__metadata__\",\n            \"__request__\",\n            \"__model__\",\n        }\n        new_parameters = []\n\n        for name, parameter in original_sig.parameters.items():\n            # Exclude keyword arguments that are frozen\n            if name in frozen_kwargs and parameter.kind in (\n                inspect.Parameter.POSITIONAL_OR_KEYWORD,\n                inspect.Parameter.KEYWORD_ONLY,\n            ):\n                continue\n            # Keep remaining parameters\n            new_parameters.append(parameter)\n\n        new_sig = inspect.Signature(\n            parameters=new_parameters, return_annotation=original_sig.return_annotation\n        )\n\n        # Ensure name, docstring and signature are properly set\n        update_wrapper(wrapper, bound_callable)\n        wrapper.__signature__ = new_sig\n\n        return wrapper\n\n    def _configure_generation(\n        self,\n        body: Dict[str, Any],\n        system_instruction: Optional[str],\n        __metadata__: Dict[str, Any],\n        __tools__: dict[str, Any] | None = None,\n    ) -> types.GenerateContentConfig:\n        \"\"\"\n        Configure generation parameters and safety settings.\n\n        Args:\n            body: The request body containing generation parameters\n            system_instruction: Optional system instruction string\n\n        Returns:\n            types.GenerateContentConfig\n        \"\"\"\n        gen_config_params = {\n            \"temperature\": body.get(\"temperature\"),\n            \"top_p\": body.get(\"top_p\"),\n            \"top_k\": body.get(\"top_k\"),\n            \"max_output_tokens\": body.get(\"max_tokens\"),\n            \"stop_sequences\": body.get(\"stop\") or None,\n            \"system_instruction\": system_instruction,\n        }\n        # Configure safety settings\n        if self.valves.USE_PERMISSIVE_SAFETY:\n            safety_settings = [\n                types.SafetySetting(\n                    category=\"HARM_CATEGORY_HARASSMENT\", threshold=\"BLOCK_NONE\"\n                ),\n                types.SafetySetting(\n                    category=\"HARM_CATEGORY_HATE_SPEECH\", threshold=\"BLOCK_NONE\"\n                ),\n                types.SafetySetting(\n                    category=\"HARM_CATEGORY_SEXUALLY_EXPLICIT\", threshold=\"BLOCK_NONE\"\n                ),\n                types.SafetySetting(\n                    category=\"HARM_CATEGORY_DANGEROUS_CONTENT\", threshold=\"BLOCK_NONE\"\n                ),\n            ]\n            gen_config_params |= {\"safety_settings\": safety_settings}\n\n        features = __metadata__.get(\"features\", {})\n        if features.get(\"google_search_tool\", False):\n            self.log.debug(\"Enabling Google search grounding\")\n            gen_config_params.setdefault(\"tools\", []).append(\n                types.Tool(google_search=types.GoogleSearch())\n            )\n\n        if __tools__ is not None and __metadata__.get(\"function_calling\") == \"native\":\n            for name, tool_def in __tools__.items():\n                tool = self._create_tool(tool_def)\n                self.log.debug(\n                    f\"Adding tool '{name}' with signature {tool.__signature__}\"\n                )\n\n                gen_config_params.setdefault(\"tools\", []).append(tool)\n\n        # Filter out None values for generation config\n        filtered_params = {k: v for k, v in gen_config_params.items() if v is not None}\n        return types.GenerateContentConfig(**filtered_params)\n\n    @staticmethod\n    def _format_grounding_chunks_as_sources(\n        grounding_chunks: list[types.GroundingChunk],\n    ):\n        formatted_sources = []\n        for chunk in grounding_chunks:\n            context = chunk.web or chunk.retrieved_context\n            if not context:\n                continue\n\n            uri = context.uri\n            title = context.title or \"Source\"\n\n            formatted_sources.append(\n                {\n                    \"source\": {\n                        \"name\": title,\n                        \"type\": \"web_search_results\",\n                        \"url\": uri,\n                    },\n                    \"document\": [\"Click the link to view the content.\"],\n                    \"metadata\": [{\"source\": title}],\n                }\n            )\n        return formatted_sources\n\n    async def _process_grounding_metadata(\n        self,\n        grounding_metadata_list: List[types.GroundingMetadata],\n        text: str,\n        __event_emitter__: Callable,\n    ):\n        \"\"\"Process and emit grounding metadata events.\"\"\"\n        grounding_chunks = []\n        web_search_queries = []\n        grounding_supports = []\n\n        for metadata in grounding_metadata_list:\n            if metadata.grounding_chunks:\n                grounding_chunks.extend(metadata.grounding_chunks)\n            if metadata.web_search_queries:\n                web_search_queries.extend(metadata.web_search_queries)\n            if metadata.grounding_supports:\n                grounding_supports.extend(metadata.grounding_supports)\n\n        # Add sources to the response\n        if grounding_chunks:\n            sources = self._format_grounding_chunks_as_sources(grounding_chunks)\n            await __event_emitter__(\n                {\"type\": \"chat:completion\", \"data\": {\"sources\": sources}}\n            )\n\n        # Add status specifying google queries used for grounding\n        if web_search_queries:\n            await __event_emitter__(\n                {\n                    \"type\": \"status\",\n                    \"data\": {\n                        \"action\": \"web_search\",\n                        \"description\": \"This response was grounded with Google Search\",\n                        \"urls\": [\n                            f\"https://www.google.com/search?q={query}\"\n                            for query in web_search_queries\n                        ],\n                    },\n                }\n            )\n\n        # Add citations in the text body\n        if grounding_supports:\n            # Citation indexes are in bytes\n            ENCODING = \"utf-8\"\n            text_bytes = text.encode(ENCODING)\n            last_byte_index = 0\n            cited_chunks = []\n\n            for support in grounding_supports:\n                cited_chunks.append(\n                    text_bytes[last_byte_index : support.segment.end_index].decode(\n                        ENCODING\n                    )\n                )\n\n                # Generate and append citations (e.g., \"[1][2]\")\n                footnotes = \"\".join(\n                    [f\"[{i + 1}]\" for i in support.grounding_chunk_indices]\n                )\n                cited_chunks.append(f\" {footnotes}\")\n\n                # Update index for the next segment\n                last_byte_index = support.segment.end_index\n\n            # Append any remaining text after the last citation\n            if last_byte_index < len(text_bytes):\n                cited_chunks.append(text_bytes[last_byte_index:].decode(ENCODING))\n\n            await __event_emitter__(\n                {\n                    \"type\": \"replace\",\n                    \"data\": {\"content\": \"\".join(cited_chunks)},\n                }\n            )\n\n    async def _handle_streaming_response(\n        self,\n        response_iterator: Any,\n        __event_emitter__: Callable,\n    ) -> AsyncIterator[str]:\n        \"\"\"\n        Handle streaming response from Gemini API.\n\n        Args:\n            response_iterator: Iterator from generate_content\n\n        Returns:\n            Generator yielding text chunks\n        \"\"\"\n        grounding_metadata_list = []\n        text_chunks = []\n\n        prompt_tokens = completion_tokens = thinking_tokens = total_tokens = (\n            cached_tokens\n        ) = None\n\n        try:\n            async for chunk in response_iterator:\n                # Check for safety feedback or empty chunks\n                if not chunk.candidates:\n                    # Check prompt feedback\n                    if (\n                        response_iterator.prompt_feedback\n                        and response_iterator.prompt_feedback.block_reason\n                    ):\n                        yield f\"[Blocked due to Prompt Safety: {response_iterator.prompt_feedback.block_reason.name}]\"\n                    else:\n                        yield \"[Blocked by safety settings]\"\n                    return  # Stop generation\n\n                if chunk.candidates[0].grounding_metadata:\n                    grounding_metadata_list.append(\n                        chunk.candidates[0].grounding_metadata\n                    )\n\n                if chunk.text:\n                    text_chunks.append(chunk.text)\n                    await __event_emitter__(\n                        {\n                            \"type\": \"chat:message:delta\",\n                            \"data\": {\n                                \"content\": chunk.text,\n                            },\n                        }\n                    )\n\n                if chunk.usage_metadata:  # appears once, at the end\n                    prompt_tokens = chunk.usage_metadata.prompt_token_count\n                    completion_tokens = (\n                        chunk.usage_metadata.candidates_token_count or 0\n                    ) + (chunk.usage_metadata.thoughts_token_count or 0)\n                    thinking_tokens = chunk.usage_metadata.thoughts_token_count\n                    cached_tokens = chunk.usage_metadata.cached_content_token_count\n                    total_tokens = chunk.usage_metadata.total_token_count\n\n            # After processing all chunks, handle grounding data\n            if grounding_metadata_list and __event_emitter__:\n                await self._process_grounding_metadata(\n                    grounding_metadata_list, \"\".join(text_chunks), __event_emitter__\n                )\n\n            usage_dict = self._build_usage_dict(\n                prompt_tokens,\n                completion_tokens,\n                thinking_tokens,\n                cached_tokens,\n                total_tokens,\n            )\n            if usage_dict:\n                yield {\"usage\": usage_dict}\n\n        except Exception as e:\n            self.log.exception(f\"Error during streaming: {e}\")\n            yield f\"Error during streaming: {e}\"\n\n    def _handle_standard_response(self, response: Any) -> str:\n        \"\"\"\n        Handle non-streaming response from Gemini API.\n\n        Args:\n            response: Response from generate_content\n\n        Returns:\n            Generated text or error message\n        \"\"\"\n        # Check for prompt safety blocks\n        if response.prompt_feedback and response.prompt_feedback.block_reason:\n            return f\"[Blocked due to Prompt Safety: {response.prompt_feedback.block_reason.name}]\"\n\n        # Check for missing candidates\n        if not response.candidates:\n            return \"[Blocked by safety settings or no candidates generated]\"\n\n        # Check candidate finish reason\n        candidate = response.candidates[0]\n        if candidate.finish_reason == types.FinishReason.SAFETY:\n            # Try to get specific safety rating info\n            blocking_rating = next(\n                (r for r in candidate.safety_ratings if r.blocked), None\n            )\n            reason = f\" ({blocking_rating.category.name})\" if blocking_rating else \"\"\n            return f\"[Blocked by safety settings{reason}]\"\n\n        # Process content parts\n        if candidate.content and candidate.content.parts:\n            # Combine text from all parts\n            return \"\".join(\n                part.text for part in candidate.content.parts if hasattr(part, \"text\")\n            )\n        else:\n            return \"[No content generated or unexpected response structure]\"\n\n    async def _retry_with_backoff(self, func, *args, **kwargs) -> Any:\n        \"\"\"\n        Retry a function with exponential backoff.\n\n        Args:\n            func: Async function to retry\n            *args, **kwargs: Arguments to pass to the function\n\n        Returns:\n            Result from the function\n\n        Raises:\n            The last exception encountered after all retries\n        \"\"\"\n        max_retries = self.valves.RETRY_COUNT\n        retry_count = 0\n        last_exception = None\n\n        while retry_count <= max_retries:\n            try:\n                return await func(*args, **kwargs)\n            except ServerError as e:\n                # These errors might be temporary, so retry\n                retry_count += 1\n                last_exception = e\n\n                if retry_count <= max_retries:\n                    # Calculate backoff time (exponential with jitter)\n                    wait_time = min(2**retry_count + (0.1 * retry_count), 10)\n                    self.log.warning(\n                        f\"Temporary error from Google API: {e}. Retrying in {wait_time:.1f}s ({retry_count}/{max_retries})\"\n                    )\n                    await asyncio.sleep(wait_time)\n                else:\n                    raise\n            except Exception:\n                # Don't retry other exceptions\n                raise\n\n        # If we get here, we've exhausted retries\n        assert last_exception is not None\n        raise last_exception\n\n    def _build_usage_dict(\n        self,\n        prompt_tokens,\n        completion_tokens,\n        thinking_tokens,\n        cached_tokens,\n        total_tokens,\n    ):\n        \"\"\"Build usage dictionary with proper structure, excluding zero values.\"\"\"\n        usage = {}\n\n        if prompt_tokens is not None:\n            usage[\"prompt_tokens\"] = prompt_tokens\n        if completion_tokens is not None:\n            usage[\"completion_tokens\"] = completion_tokens\n        if total_tokens is not None:\n            usage[\"total_tokens\"] = total_tokens\n\n        # Build prompt_tokens_details if there are non-zero values\n        prompt_details = {}\n        if cached_tokens and cached_tokens > 0:\n            prompt_details[\"cached_tokens\"] = cached_tokens\n        if prompt_details:\n            usage[\"prompt_tokens_details\"] = prompt_details\n\n        # Build completion_tokens_details if there are non-zero values\n        completion_details = {}\n        if thinking_tokens and thinking_tokens > 0:\n            completion_details[\"reasoning_tokens\"] = thinking_tokens\n        if completion_details:\n            usage[\"completion_tokens_details\"] = completion_details\n\n        return usage\n\n    async def pipe(\n        self,\n        body: Dict[str, Any],\n        __metadata__: dict[str, Any],\n        __event_emitter__: Callable,\n        __tools__: dict[str, Any] | None,\n    ) -> Union[str, AsyncIterator[str]]:\n        \"\"\"\n        Main method for sending requests to the Google Gemini endpoint.\n\n        Args:\n            body: The request body containing messages and other parameters.\n\n        Returns:\n            Response from Google Gemini API, which could be a string or an iterator for streaming.\n        \"\"\"\n        # Setup logging for this request\n        request_id = id(body)\n        self.log.debug(f\"Processing request {request_id}\")\n\n        try:\n            # Parse and validate model ID\n            model_id = body.get(\"model\", \"\")\n            try:\n                model_id = self._prepare_model_id(model_id)\n                self.log.debug(f\"Using model: {model_id}\")\n            except ValueError as ve:\n                return f\"Model Error: {ve}\"\n\n            # Get stream flag\n            stream = body.get(\"stream\", False)\n            messages = body.get(\"messages\", [])\n\n            # Prepare content and extract system message\n            contents, system_instruction = self._prepare_content(messages)\n            if not contents:\n                return \"Error: No valid message content found\"\n\n            # Configure generation parameters and safety settings\n            generation_config = self._configure_generation(\n                body, system_instruction, __metadata__, __tools__\n            )\n\n            # Make the API call\n            client = self._get_client()\n            if stream:\n                try:\n\n                    async def get_streaming_response():\n                        return await client.aio.models.generate_content_stream(\n                            model=model_id,\n                            contents=contents,\n                            config=generation_config,\n                        )\n\n                    response_iterator = await self._retry_with_backoff(\n                        get_streaming_response\n                    )\n                    self.log.debug(f\"Request {request_id}: Got streaming response\")\n                    return self._handle_streaming_response(\n                        response_iterator, __event_emitter__\n                    )\n\n                except Exception as e:\n                    self.log.exception(f\"Error in streaming request {request_id}: {e}\")\n                    return f\"Error during streaming: {e}\"\n            else:\n                try:\n\n                    async def get_response():\n                        return await client.aio.models.generate_content(\n                            model=model_id,\n                            contents=contents,\n                            config=generation_config,\n                        )\n\n                    response = await self._retry_with_backoff(get_response)\n                    self.log.debug(f\"Request {request_id}: Got non-streaming response\")\n                    return self._handle_standard_response(response)\n\n                except Exception as e:\n                    self.log.exception(\n                        f\"Error in non-streaming request {request_id}: {e}\"\n                    )\n                    return f\"Error generating content: {e}\"\n\n        except ClientError as ce:\n            error_msg = f\"Client error raised by the GenAI API: {ce}.\"\n            self.log.error(f\"Client error: {ce}\")\n            return error_msg\n\n        except ServerError as se:\n            error_msg = f\"Server error raised by the GenAI API: {se}\"\n            self.log.error(f\"Server error raised by the GenAI API.: {se}\")\n            return error_msg\n\n        except APIError as apie:\n            error_msg = f\"Google API Error: {apie}\"\n            self.log.error(error_msg)\n            return error_msg\n\n        except ValueError as ve:\n            error_msg = f\"Configuration error: {ve}\"\n            self.log.error(f\"Value error: {ve}\")\n            return error_msg\n\n        except Exception as e:\n            # Log the full error with traceback\n            import traceback\n\n            error_trace = traceback.format_exc()\n            self.log.exception(f\"Unexpected error: {e}\\n{error_trace}\")\n\n            # Return a user-friendly error message\n            return f\"An error occurred while processing your request: {e}\"\n",
        "meta": {
            "description": "A manifold pipeline for interacting with Google Gemini models, including dynamic model specification, streaming responses, and flexible error handling.",
            "manifest": {
                "title": "Google Gemini Pipeline",
                "author": "owndev, olivier-lacroix",
                "author_url": "https://github.com/owndev/",
                "project_url": "https://github.com/owndev/Open-WebUI-Functions",
                "funding_url": "https://github.com/sponsors/owndev",
                "version": "1.3.3",
                "license": "Apache License 2.0",
                "description": "A manifold pipeline for interacting with Google Gemini models, including dynamic model specification, streaming responses, and flexible error handling.",
                "features": ""
            }
        },
        "is_active": true,
        "is_global": false,
        "updated_at": 1757968249,
        "created_at": 1757946487
    },
    {
        "id": "credit_management_models",
        "user_id": "d905e88b-549b-494e-95c8-ca3224fcaa2e",
        "name": "Credit management Models",
        "type": "action",
        "content": "\"\"\"\ntitle: Credit management Models\nauthor: Miloslav Konop√≠k, DDVVY\nversion: 1.0\n\"\"\"\n\nimport os\nfrom pydantic import BaseModel, Field\nimport httpx\n\n# Translation table for i18n support\nTRANSLATIONS = {\n    \"cs-CZ\": {\n        \"failed_to_load_pricing\": \"Nepoda≈ôilo se naƒç√≠st cen√≠k modelu: {}\",\n        \"model_not_found_pricing\": \"Model nenalezen v cen√≠ku.\",\n        \"model_pricing_title\": \"üìä Cen√≠k modelu **{}**:\",\n        \"free_model_pricing\": \"üÜì **BEZPLATN√ù MODEL** - ≈Ω√°dn√© kredity nejsou vy≈æadov√°ny\",\n        \"prompt_price\": \"‚Ä¢ Prompt (vstup): {} kredit≈Ø/token\",\n        \"completion_price\": \"‚Ä¢ Dokonƒçen√≠ (v√Ωstup): {} kredit≈Ø/token\",\n        \"prompt_price_1m\": \"‚Ä¢ Prompt (vstup, 1M token≈Ø): {} kredit≈Ø\",\n        \"completion_price_1m\": \"‚Ä¢ Dokonƒçen√≠ (v√Ωstup, 1M token≈Ø): {} kredit≈Ø\",\n    },\n    \"en\": {  # Fallback language\n        \"failed_to_load_pricing\": \" Failed to load model pricing: {}\",\n        \"model_not_found_pricing\": \" Model not found in pricing list.\",\n        \"model_pricing_title\": \"üìä Model **{}** pricing:\",\n        \"free_model_pricing\": \"üÜì **FREE MODEL** - No credits required\",\n        \"prompt_price\": \"‚Ä¢ Prompt (input): {} credits/token\",\n        \"completion_price\": \"‚Ä¢ Completion (output): {} credits/token\",\n        \"prompt_price_1m\": \"‚Ä¢ Prompt (input, 1M tokens): {} credits\",\n        \"completion_price_1m\": \"‚Ä¢ Completion (output, 1M tokens): {} credits\",\n    },\n}\n\n\nclass Action:\n    class Valves(BaseModel):\n        show_status: bool = Field(\n            default=False, description=\"Not used ‚Äì reserved for future options\"\n        )\n        credits_api_protocol: str = Field(\n            default=\"http\", description=\"API protocol (http or https)\"\n        )\n        credits_api_host: str = Field(\n            default=\"localhost:8000\", description=\"API host and port\"\n        )\n        ssl_verify: bool = Field(default=False, description=\"Verify SSL certificates\")\n        api_key: str = Field(default=\"\", description=\"API key for authentication\")\n\n    def __init__(self):\n        self.valves = self.Valves()\n\n    def _get_user_language(self, body):\n        \"\"\"Extract user language from body metadata\"\"\"\n        try:\n            return (\n                body.get(\"metadata\", {})\n                .get(\"variables\", {})\n                .get(\"{{USER_LANGUAGE}}\", \"en\")\n            )\n        except:\n            return \"en\"\n\n    def _translate(self, key, lang=\"en\", **kwargs):\n        \"\"\"Get translated string for given key and language\"\"\"\n        # Get the translation for the specific language, fallback to English\n        translation = TRANSLATIONS.get(lang, TRANSLATIONS[\"en\"]).get(\n            key, TRANSLATIONS[\"en\"].get(key, key)\n        )\n\n        # Ensure we have a valid translation string\n        if translation is None:\n            translation = key\n\n        # Format the translation with any provided kwargs\n        try:\n            return translation.format(**kwargs)\n        except:\n            return translation\n\n    def _format_credits(self, value):\n        \"\"\"Format credit values with commas and trim unnecessary decimals.\"\"\"\n        try:\n            s = f\"{value:,.6f}\"\n            s = s.rstrip(\"0\").rstrip(\".\")\n            return s\n        except:\n            return str(value)\n\n    async def action(\n        self, body, __user__=None, __event_emitter__=None, __event_call__=None\n    ):\n        credits_api_base_url = f\"{self.valves.credits_api_protocol}://{self.valves.credits_api_host}/api/credits\"\n        model_name = body.get(\"model\", \"\")\n\n        # Get user language for translations\n        user_lang = self._get_user_language(body)\n\n        try:\n            # Set up headers with API key\n            headers = {\"X-API-Key\": self.valves.api_key} if self.valves.api_key else {}\n\n            async with httpx.AsyncClient(verify=self.valves.ssl_verify) as client:\n                # Use optimized endpoint for specific model\n                res = await client.get(\n                    f\"{credits_api_base_url}/model/{model_name}\", headers=headers\n                )\n                res.raise_for_status()\n                model_data = res.json()\n        except Exception as e:\n            body[\"messages\"][-1][\n                \"content\"\n            ] += f\"\\n\\n{self._translate('failed_to_load_pricing', user_lang).format(str(e))}\"\n            return body\n\n        if not model_data:\n            body[\"messages\"][-1][\n                \"content\"\n            ] += f\"\\n\\n{self._translate('model_not_found_pricing', user_lang)}\"\n            return body\n\n        context_price = model_data.get(\"context_price\", 0)\n        generation_price = model_data.get(\"generation_price\", 0)\n        is_free = model_data.get(\"is_free\", False)\n\n        if is_free:\n            body[\"messages\"][-1][\"content\"] += (\n                f\"\\n\\n{self._translate('model_pricing_title', user_lang).format(model_name)}\\n\"\n                f\"{self._translate('free_model_pricing', user_lang)}\"\n            )\n        else:\n            # Multiply per-token prices by 1,000,000 and format\n            context_price_1m = context_price * 1_000_000\n            generation_price_1m = generation_price * 1_000_000\n\n            body[\"messages\"][-1][\"content\"] += (\n                f\"\\n\\n{self._translate('model_pricing_title', user_lang).format(model_name)}\\n\"\n                f\"{self._translate('prompt_price_1m', user_lang).format(self._format_credits(context_price_1m))}\\n\"\n                f\"{self._translate('completion_price_1m', user_lang).format(self._format_credits(generation_price_1m))}\"\n            )\n\n        return body\n",
        "meta": {
            "description": "Showing model price",
            "manifest": {
                "title": "Credit management Models",
                "author": "Miloslav Konop√≠k, DDVVY",
                "version": "1.0"
            }
        },
        "is_active": true,
        "is_global": true,
        "updated_at": 1758053185,
        "created_at": 1757946454
    }
]